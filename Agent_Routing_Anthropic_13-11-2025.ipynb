{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MudMonster341/Agentic_AI/blob/main/Agent_Routing_Anthropic_13-11-2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade anthropic\n",
        "!pip install nest_asyncio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vi0HU0mVp_2",
        "outputId": "096232a3-1500-4425-86f0-fdac0acdd00e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anthropic\n",
            "  Downloading anthropic-0.72.1-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.17.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from anthropic) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.2)\n",
            "Downloading anthropic-0.72.1-py3-none-any.whl (357 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/357.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m357.4/357.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: anthropic\n",
            "Successfully installed anthropic-0.72.1\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import anthropic\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "import time\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from enum import Enum\n",
        "from typing import List, Optional, Dict\n",
        "from threading import Lock\n",
        "from collections import defaultdict\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Enable nested asyncio for Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "print(\"‚úì All imports successful!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8p4tJqxWTvj",
        "outputId": "02a2001e-7d0e-4f16-d224-1842d8d565ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì All imports successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Secure API key input for Colab\n",
        "from google.colab import userdata\n",
        "\n",
        "# Option 1: Use Colab Secrets (Recommended)\n",
        "# Go to the key icon (üîë) in left sidebar > Add Secret > Name: ANTHROPIC_API_KEY\n",
        "try:\n",
        "    api_key = userdata.get('ANTHROPIC_API_KEY')\n",
        "    print(\"‚úì API key loaded from Colab Secrets\")\n",
        "except:\n",
        "    # Option 2: Manual input (will be hidden)\n",
        "    api_key = getpass(\"Enter your Anthropic API Key: \")\n",
        "    print(\"‚úì API key entered manually\")\n",
        "\n",
        "# Validate API key format\n",
        "if not api_key or not api_key.startswith('sk-ant-'):\n",
        "    raise ValueError(\"Invalid API key format. Should start with 'sk-ant-'\")\n",
        "\n",
        "# Initialize Anthropic clients\n",
        "client = anthropic.Anthropic(api_key=api_key)\n",
        "async_client = anthropic.AsyncAnthropic(api_key=api_key)\n",
        "\n",
        "print(\"‚úì Anthropic client initialized successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8vynwXzWVlr",
        "outputId": "db1e5a9c-9ad7-440c-e745-ec034a54c500"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì API key loaded from Colab Secrets\n",
            "‚úì Anthropic client initialized successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentType(Enum):\n",
        "    ORDER_STATUS = \"order_status\"\n",
        "    REFUND = \"refund\"\n",
        "    TECHNICAL_SUPPORT = \"technical_support\"\n",
        "    ESCALATION = \"escalation\"\n",
        "\n",
        "class QueryType(Enum):\n",
        "    TRACKING = \"tracking\"\n",
        "    REFUND = \"refund\"\n",
        "    TECHNICAL = \"technical\"\n",
        "    ESCALATION = \"escalation\"\n",
        "\n",
        "@dataclass\n",
        "class Query:\n",
        "    id: int\n",
        "    content: str\n",
        "    query_type: QueryType\n",
        "    priority: int  # 1-5, 5 being highest\n",
        "    timestamp: float\n",
        "    customer_context: Optional[str] = None\n",
        "\n",
        "@dataclass\n",
        "class AgentCapability:\n",
        "    agent_type: AgentType\n",
        "    model_name: str  # Claude model to use\n",
        "    max_tokens: int\n",
        "    temperature: float\n",
        "    system_prompt: str\n",
        "    max_concurrent_tasks: int\n",
        "    capabilities: List[QueryType]\n",
        "\n",
        "print(\"‚úì Data structures defined\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlhxwI6gWany",
        "outputId": "aabcc165-9850-445c-d643-ca469b0ff16d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Data structures defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ClaudeAgent:\n",
        "    \"\"\"Agent powered by actual Claude API\"\"\"\n",
        "\n",
        "    def __init__(self, agent_id: int, capability: AgentCapability, async_client):\n",
        "        self.agent_id = agent_id\n",
        "        self.capability = capability\n",
        "        self.async_client = async_client\n",
        "        self.current_tasks = 0\n",
        "        self.completed_tasks = 0\n",
        "        self.failed_tasks = 0\n",
        "        self.total_processing_time = 0.0\n",
        "        self.lock = Lock()\n",
        "\n",
        "    def is_available(self) -> bool:\n",
        "        with self.lock:\n",
        "            return self.current_tasks < self.capability.max_concurrent_tasks\n",
        "\n",
        "    def can_handle(self, query: Query) -> bool:\n",
        "        return query.query_type in self.capability.capabilities\n",
        "\n",
        "    async def process_query(self, query: Query) -> dict:\n",
        "        \"\"\"Process query using Claude API\"\"\"\n",
        "        with self.lock:\n",
        "            self.current_tasks += 1\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            # Construct messages for Claude\n",
        "            user_message = f\"\"\"Customer Query: {query.content}\n",
        "\n",
        "Query Type: {query.query_type.value}\n",
        "Priority: {query.priority}/5\n",
        "{f'Context: {query.customer_context}' if query.customer_context else ''}\n",
        "\n",
        "Please provide a helpful, professional response to this customer query.\"\"\"\n",
        "\n",
        "            # Make async API call to Claude\n",
        "            message = await self.async_client.messages.create(\n",
        "                model=self.capability.model_name,\n",
        "                max_tokens=self.capability.max_tokens,\n",
        "                temperature=self.capability.temperature,\n",
        "                system=self.capability.system_prompt,\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": user_message}\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            # Extract response\n",
        "            response_text = message.content[0].text\n",
        "            processing_time = time.time() - start_time\n",
        "\n",
        "            with self.lock:\n",
        "                self.current_tasks -= 1\n",
        "                self.completed_tasks += 1\n",
        "                self.total_processing_time += processing_time\n",
        "\n",
        "            return {\n",
        "                'success': True,\n",
        "                'response': response_text,\n",
        "                'processing_time': processing_time,\n",
        "                'model': self.capability.model_name,\n",
        "                'tokens_used': message.usage.input_tokens + message.usage.output_tokens\n",
        "            }\n",
        "\n",
        "        except anthropic.RateLimitError as e:\n",
        "            # Handle rate limiting\n",
        "            with self.lock:\n",
        "                self.current_tasks -= 1\n",
        "                self.failed_tasks += 1\n",
        "\n",
        "            return {\n",
        "                'success': False,\n",
        "                'error': 'rate_limit',\n",
        "                'message': 'API rate limit exceeded',\n",
        "                'processing_time': time.time() - start_time\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            with self.lock:\n",
        "                self.current_tasks -= 1\n",
        "                self.failed_tasks += 1\n",
        "\n",
        "            return {\n",
        "                'success': False,\n",
        "                'error': str(type(e).__name__),\n",
        "                'message': str(e),\n",
        "                'processing_time': time.time() - start_time\n",
        "            }\n",
        "\n",
        "    def get_stats(self) -> dict:\n",
        "        with self.lock:\n",
        "            total = self.completed_tasks + self.failed_tasks\n",
        "            return {\n",
        "                'agent_id': self.agent_id,\n",
        "                'type': self.capability.agent_type.value,\n",
        "                'model': self.capability.model_name,\n",
        "                'completed': self.completed_tasks,\n",
        "                'failed': self.failed_tasks,\n",
        "                'success_rate': self.completed_tasks / total if total > 0 else 0,\n",
        "                'avg_processing_time': self.total_processing_time / total if total > 0 else 0,\n",
        "                'current_load': self.current_tasks\n",
        "            }\n",
        "\n",
        "print(\"‚úì ClaudeAgent class defined\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8yuh-3GWpw8",
        "outputId": "eb4bc177-329b-491e-9eec-779f8ca0a988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì ClaudeAgent class defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_claude_agent_pool(async_client) -> List[ClaudeAgent]:\n",
        "    \"\"\"Create pool of Claude-powered agents with different specializations\"\"\"\n",
        "    agents = []\n",
        "\n",
        "    # Order Status Agents - Fast responses with Claude 3.5 Haiku\n",
        "    for i in range(2):\n",
        "        agents.append(ClaudeAgent(\n",
        "            agent_id=i,\n",
        "            capability=AgentCapability(\n",
        "                agent_type=AgentType.ORDER_STATUS,\n",
        "                model_name=\"claude-3-5-haiku-20241022\",  # Fast & cost-effective\n",
        "                max_tokens=300,\n",
        "                temperature=0.3,\n",
        "                system_prompt=\"\"\"You are an Order Status Agent for an e-commerce platform.\n",
        "Your role is to provide quick, accurate information about order tracking and delivery status.\n",
        "Be concise, friendly, and reassuring. Always include estimated delivery dates when applicable.\"\"\",\n",
        "                max_concurrent_tasks=3,\n",
        "                capabilities=[QueryType.TRACKING]\n",
        "            ),\n",
        "            async_client=async_client\n",
        "        ))\n",
        "\n",
        "    # Refund Agents - Moderate complexity with Claude 3.5 Sonnet\n",
        "    for i in range(2, 4):\n",
        "        agents.append(ClaudeAgent(\n",
        "            agent_id=i,\n",
        "            capability=AgentCapability(\n",
        "                agent_type=AgentType.REFUND,\n",
        "                model_name=\"claude-3-5-sonnet-20241022\",  # Balanced performance\n",
        "                max_tokens=500,\n",
        "                temperature=0.2,\n",
        "                system_prompt=\"\"\"You are a Refund Processing Agent for an e-commerce platform.\n",
        "Handle refund requests professionally, explain the refund process clearly, and mention approval timelines.\n",
        "Be empathetic but follow policy guidelines. Mention: processing takes 3-5 business days.\"\"\",\n",
        "                max_concurrent_tasks=2,\n",
        "                capabilities=[QueryType.REFUND]\n",
        "            ),\n",
        "            async_client=async_client\n",
        "        ))\n",
        "\n",
        "    # Technical Support Agents - Complex issues with Claude 3.5 Sonnet\n",
        "    for i in range(4, 5):\n",
        "        agents.append(ClaudeAgent(\n",
        "            agent_id=i,\n",
        "            capability=AgentCapability(\n",
        "                agent_type=AgentType.TECHNICAL_SUPPORT,\n",
        "                model_name=\"claude-3-5-sonnet-20241022\",\n",
        "                max_tokens=800,\n",
        "                temperature=0.1,\n",
        "                system_prompt=\"\"\"You are a Technical Support Agent for an e-commerce platform.\n",
        "Troubleshoot complex technical issues including app crashes, payment failures, and website errors.\n",
        "Provide step-by-step solutions. Be patient and thorough. Escalate if issue requires developer intervention.\"\"\",\n",
        "                max_concurrent_tasks=2,\n",
        "                capabilities=[QueryType.TECHNICAL]\n",
        "            ),\n",
        "            async_client=async_client\n",
        "        ))\n",
        "\n",
        "    # Escalation Agent - Handles complaints with Claude 3.5 Sonnet (best quality)\n",
        "    agents.append(ClaudeAgent(\n",
        "        agent_id=5,\n",
        "        capability=AgentCapability(\n",
        "            agent_type=AgentType.ESCALATION,\n",
        "            model_name=\"claude-3-5-sonnet-20241022\",\n",
        "            max_tokens=700,\n",
        "            temperature=0.4,\n",
        "            system_prompt=\"\"\"You are an Escalation Agent handling sensitive customer complaints.\n",
        "Be extremely empathetic, acknowledge frustrations, and offer concrete solutions or compensation.\n",
        "You can handle any query type in escalation mode. Prioritize customer satisfaction and retention.\"\"\",\n",
        "            max_concurrent_tasks=2,\n",
        "            capabilities=[QueryType.ESCALATION, QueryType.REFUND, QueryType.TECHNICAL, QueryType.TRACKING]\n",
        "        ),\n",
        "        async_client=async_client\n",
        "    ))\n",
        "\n",
        "    return agents\n",
        "\n",
        "print(\"‚úì Agent factory defined\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEi6ecsSWzCX",
        "outputId": "1a346c3d-cd27-4ca7-dcd3-4080451d62d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Agent factory defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class Router(ABC):\n",
        "    \"\"\"Abstract base class for routing strategies\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def route(self, query: Query, agents: List[ClaudeAgent]) -> Optional[ClaudeAgent]:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_name(self) -> str:\n",
        "        pass\n",
        "\n",
        "class CapabilityMatchingRouter(Router):\n",
        "    \"\"\"Routes based on agent capabilities and performance\"\"\"\n",
        "\n",
        "    def route(self, query: Query, agents: List[ClaudeAgent]) -> Optional[ClaudeAgent]:\n",
        "        scored_agents = []\n",
        "\n",
        "        for agent in agents:\n",
        "            if not agent.can_handle(query):\n",
        "                continue\n",
        "\n",
        "            # Calculate comprehensive score\n",
        "            availability_score = 1.0 if agent.is_available() else 0.3\n",
        "\n",
        "            stats = agent.get_stats()\n",
        "            success_score = stats['success_rate'] if stats['success_rate'] > 0 else 0.7\n",
        "\n",
        "            # Model speed approximation (Haiku faster than Sonnet)\n",
        "            speed_score = 1.0 if 'haiku' in agent.capability.model_name.lower() else 0.7\n",
        "\n",
        "            load_score = 1.0 - (agent.current_tasks / agent.capability.max_concurrent_tasks)\n",
        "\n",
        "            # Weighted combination\n",
        "            total_score = (\n",
        "                availability_score * 0.4 +\n",
        "                success_score * 0.3 +\n",
        "                speed_score * 0.2 +\n",
        "                load_score * 0.1\n",
        "            )\n",
        "\n",
        "            scored_agents.append((agent, total_score))\n",
        "\n",
        "        if not scored_agents:\n",
        "            # Fallback: try escalation agent\n",
        "            escalation = [a for a in agents if a.capability.agent_type == AgentType.ESCALATION]\n",
        "            return escalation[0] if escalation else None\n",
        "\n",
        "        return max(scored_agents, key=lambda x: x[1])[0]\n",
        "\n",
        "    def get_name(self) -> str:\n",
        "        return \"Capability-Matching (Claude-Powered)\"\n",
        "\n",
        "class PriorityRouter(Router):\n",
        "    \"\"\"Priority-based routing with smart fallbacks\"\"\"\n",
        "\n",
        "    def route(self, query: Query, agents: List[ClaudeAgent]) -> Optional[ClaudeAgent]:\n",
        "        # High priority queries go to best available agent\n",
        "        if query.priority >= 4:\n",
        "            capable = [a for a in agents if a.can_handle(query)]\n",
        "            if capable:\n",
        "                # Prefer available agents, then least loaded\n",
        "                available = [a for a in capable if a.is_available()]\n",
        "                pool = available if available else capable\n",
        "                return min(pool, key=lambda a: a.current_tasks)\n",
        "\n",
        "        # Normal priority - standard matching\n",
        "        capable = [a for a in agents if a.can_handle(query) and a.is_available()]\n",
        "\n",
        "        if not capable:\n",
        "            # All busy - find escalation\n",
        "            escalation = [a for a in agents if a.capability.agent_type == AgentType.ESCALATION]\n",
        "            return escalation[0] if escalation else None\n",
        "\n",
        "        return random.choice(capable)\n",
        "\n",
        "    def get_name(self) -> str:\n",
        "        return \"Priority-Based (Claude-Powered)\"\n",
        "\n",
        "print(\"‚úì Routing mechanisms defined\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hA3-ZJ4NXJne",
        "outputId": "24e59b90-f5b9-403b-8e29-49b2b255db73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Routing mechanisms defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_realistic_queries(num_queries: int = 10, load_spike: bool = False) -> List[Query]:\n",
        "    \"\"\"Generate realistic customer service queries\"\"\"\n",
        "    queries = []\n",
        "\n",
        "    # Distribution based on typical e-commerce patterns\n",
        "    if load_spike:\n",
        "        weights = [0.70, 0.20, 0.05, 0.05]  # Black Friday - lots of tracking\n",
        "    else:\n",
        "        weights = [0.50, 0.30, 0.15, 0.05]  # Normal distribution\n",
        "\n",
        "    query_templates = {\n",
        "        QueryType.TRACKING: [\n",
        "            \"Where is my package? Order #12345\",\n",
        "            \"My order hasn't arrived yet. Can you check the status?\",\n",
        "            \"When will my order be delivered? It's been 5 days.\",\n",
        "            \"I need to track my shipment urgently\"\n",
        "        ],\n",
        "        QueryType.REFUND: [\n",
        "            \"I want a refund for my order. The product is defective.\",\n",
        "            \"Can I get my money back? This isn't what I ordered.\",\n",
        "            \"How do I request a refund? Product arrived damaged.\",\n",
        "            \"I'd like to return this item and get a refund\"\n",
        "        ],\n",
        "        QueryType.TECHNICAL: [\n",
        "            \"Your app crashed when I tried to checkout!\",\n",
        "            \"I can't login to my account. Getting error 500.\",\n",
        "            \"The website keeps freezing when I add items to cart.\",\n",
        "            \"Payment not processing - says invalid card but it works elsewhere\"\n",
        "        ],\n",
        "        QueryType.ESCALATION: [\n",
        "            \"This is unacceptable! I've been waiting 3 weeks!\",\n",
        "            \"I want to speak to a manager NOW. This is ridiculous.\",\n",
        "            \"Worst customer service ever. I'm filing a complaint.\",\n",
        "            \"I demand compensation for this terrible experience!\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    contexts = [\n",
        "        \"Customer has ordered 5 times before\",\n",
        "        \"First-time customer\",\n",
        "        \"Premium member since 2023\",\n",
        "        \"Had previous issue resolved successfully\",\n",
        "        None\n",
        "    ]\n",
        "\n",
        "    query_types = list(QueryType)\n",
        "\n",
        "    for i in range(num_queries):\n",
        "        qtype = random.choices(query_types, weights=weights)[0]\n",
        "        template = random.choice(query_templates[qtype])\n",
        "        priority = 5 if qtype == QueryType.ESCALATION else random.randint(1, 4)\n",
        "\n",
        "        queries.append(Query(\n",
        "            id=i + 1,\n",
        "            content=template,\n",
        "            query_type=qtype,\n",
        "            priority=priority,\n",
        "            timestamp=time.time(),\n",
        "            customer_context=random.choice(contexts)\n",
        "        ))\n",
        "\n",
        "    return queries\n",
        "\n",
        "print(\"‚úì Query generator defined\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiJ9XHCYXiix",
        "outputId": "6c09b9db-e657-447f-ba7b-871f6dbb11cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Query generator defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "async def run_claude_simulation(\n",
        "    router: Router,\n",
        "    num_queries: int = 10,\n",
        "    show_responses: bool = True,\n",
        "    rate_limit_delay: float = 0.5  # Delay between requests to respect rate limits\n",
        "):\n",
        "    \"\"\"Run simulation with actual Claude API calls\"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"üöÄ RUNNING SIMULATION: {router.get_name()}\")\n",
        "    print(f\"üìä Processing {num_queries} queries with real Claude models\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    # Initialize\n",
        "    agents = create_claude_agent_pool(async_client)\n",
        "    queries = generate_realistic_queries(num_queries)\n",
        "\n",
        "    # Track metrics\n",
        "    results = []\n",
        "    total_tokens = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Process queries with rate limit consideration\n",
        "    for idx, query in enumerate(queries):\n",
        "        print(f\"\\n[Query {query.id}/{num_queries}] Type: {query.query_type.value} | Priority: {query.priority}\")\n",
        "        print(f\"  üìù Content: {query.content}\")\n",
        "\n",
        "        # Route query\n",
        "        agent = router.route(query, agents)\n",
        "\n",
        "        if agent is None:\n",
        "            print(f\"  ‚ùå No agent available\")\n",
        "            results.append({'success': False, 'reason': 'no_agent'})\n",
        "            continue\n",
        "\n",
        "        print(f\"  ‚û°Ô∏è  Routed to: Agent {agent.agent_id} ({agent.capability.agent_type.value}) using {agent.capability.model_name}\")\n",
        "\n",
        "        # Process with Claude\n",
        "        result = await agent.process_query(query)\n",
        "        results.append(result)\n",
        "\n",
        "        if result['success']:\n",
        "            total_tokens += result.get('tokens_used', 0)\n",
        "            print(f\"  ‚úÖ Success | Time: {result['processing_time']:.2f}s | Tokens: {result.get('tokens_used', 0)}\")\n",
        "\n",
        "            if show_responses:\n",
        "                response_preview = result['response'][:200] + \"...\" if len(result['response']) > 200 else result['response']\n",
        "                print(f\"  üí¨ Response: {response_preview}\\n\")\n",
        "        else:\n",
        "            print(f\"  ‚ùå Failed: {result.get('error', 'unknown')} - {result.get('message', '')}\")\n",
        "\n",
        "        # Rate limiting: Add delay between requests\n",
        "        if idx < len(queries) - 1:\n",
        "            await asyncio.sleep(rate_limit_delay)\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"üìà SIMULATION SUMMARY\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    successful = sum(1 for r in results if r.get('success', False))\n",
        "    failed = len(results) - successful\n",
        "\n",
        "    print(f\"Total Queries: {len(results)}\")\n",
        "    print(f\"‚úÖ Successful: {successful} ({successful/len(results)*100:.1f}%)\")\n",
        "    print(f\"‚ùå Failed: {failed}\")\n",
        "    print(f\"‚è±Ô∏è  Total Time: {total_time:.2f}s\")\n",
        "    print(f\"üéØ Throughput: {len(results)/total_time:.2f} queries/second\")\n",
        "    print(f\"ü™ô Total Tokens Used: {total_tokens:,}\")\n",
        "\n",
        "    # Agent performance\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"ü§ñ AGENT PERFORMANCE\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"{'ID':<4} {'Type':<20} {'Model':<30} {'Done':<6} {'Failed':<6} {'Rate':<8}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for agent in agents:\n",
        "        stats = agent.get_stats()\n",
        "        print(f\"{stats['agent_id']:<4} {stats['type']:<20} {stats['model']:<30} \"\n",
        "              f\"{stats['completed']:<6} {stats['failed']:<6} {stats['success_rate']:<8.1%}\")\n",
        "\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    return results, agents\n",
        "\n",
        "print(\"‚úì Simulation function defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeUQMjRZXzkI",
        "outputId": "64e5d87b-64ff-41f7-f487-312cd80461a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Simulation function defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run a small simulation with real Claude API calls\n",
        "# Start with just 5 queries to test\n",
        "\n",
        "router = CapabilityMatchingRouter()\n",
        "results, agents = await run_claude_simulation(\n",
        "    router=router,\n",
        "    num_queries=5,\n",
        "    show_responses=True,  # Set to False to hide detailed responses\n",
        "    rate_limit_delay=0.5  # Adjust based on your rate limits\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHKl2pD1X9Hc",
        "outputId": "376b7847-bdaf-4220-8347-0db800727b6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "üöÄ RUNNING SIMULATION: Capability-Matching (Claude-Powered)\n",
            "üìä Processing 5 queries with real Claude models\n",
            "======================================================================\n",
            "\n",
            "\n",
            "[Query 1/5] Type: refund | Priority: 4\n",
            "  üìù Content: Can I get my money back? This isn't what I ordered.\n",
            "  ‚û°Ô∏è  Routed to: Agent 2 (refund) using claude-3-5-sonnet-20241022\n",
            "  ‚ùå Failed: NotFoundError - Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-3-5-sonnet-20241022'}, 'request_id': 'req_011CV6KnHWxxJwjv3J8rz8ew'}\n",
            "\n",
            "[Query 2/5] Type: tracking | Priority: 1\n",
            "  üìù Content: Where is my package? Order #12345\n",
            "  ‚û°Ô∏è  Routed to: Agent 0 (order_status) using claude-3-5-haiku-20241022\n",
            "  ‚úÖ Success | Time: 2.58s | Tokens: 191\n",
            "  üí¨ Response: Hi there! I'll help you track Order #12345 right away.\n",
            "\n",
            "‚úÖ Order Status: Currently in transit\n",
            "üìç Current Location: Shipped from local distribution center\n",
            "üöö Estimated Delivery: Within 2-3 business days\n",
            "\n",
            "...\n",
            "\n",
            "\n",
            "[Query 3/5] Type: escalation | Priority: 5\n",
            "  üìù Content: Worst customer service ever. I'm filing a complaint.\n",
            "  ‚û°Ô∏è  Routed to: Agent 5 (escalation) using claude-3-5-sonnet-20241022\n",
            "  ‚ùå Failed: NotFoundError - Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-3-5-sonnet-20241022'}, 'request_id': 'req_011CV6KnZUevzhx1R4jfUvd2'}\n",
            "\n",
            "[Query 4/5] Type: tracking | Priority: 1\n",
            "  üìù Content: Where is my package? Order #12345\n",
            "  ‚û°Ô∏è  Routed to: Agent 0 (order_status) using claude-3-5-haiku-20241022\n",
            "  ‚úÖ Success | Time: 2.44s | Tokens: 197\n",
            "  üí¨ Response: Hi there! I'll help you track Order #12345 right away.\n",
            "\n",
            "‚úÖ Current Status: Your package is in transit and on schedule\n",
            "üìç Last Tracked Location: Shipping facility in [City]\n",
            "üöö Estimated Delivery: [Specifi...\n",
            "\n",
            "\n",
            "[Query 5/5] Type: tracking | Priority: 2\n",
            "  üìù Content: When will my order be delivered? It's been 5 days.\n",
            "  ‚û°Ô∏è  Routed to: Agent 0 (order_status) using claude-3-5-haiku-20241022\n",
            "  ‚úÖ Success | Time: 2.46s | Tokens: 186\n",
            "  üí¨ Response: Hi there! I'd be happy to help you track your order. Could you please provide me with your order number or the email address associated with the purchase? This will help me look up the specific detail...\n",
            "\n",
            "\n",
            "======================================================================\n",
            "üìà SIMULATION SUMMARY\n",
            "======================================================================\n",
            "Total Queries: 5\n",
            "‚úÖ Successful: 3 (60.0%)\n",
            "‚ùå Failed: 2\n",
            "‚è±Ô∏è  Total Time: 9.89s\n",
            "üéØ Throughput: 0.51 queries/second\n",
            "ü™ô Total Tokens Used: 574\n",
            "\n",
            "======================================================================\n",
            "ü§ñ AGENT PERFORMANCE\n",
            "======================================================================\n",
            "ID   Type                 Model                          Done   Failed Rate    \n",
            "----------------------------------------------------------------------\n",
            "0    order_status         claude-3-5-haiku-20241022      3      0      100.0%  \n",
            "1    order_status         claude-3-5-haiku-20241022      0      0      0.0%    \n",
            "2    refund               claude-3-5-sonnet-20241022     0      1      0.0%    \n",
            "3    refund               claude-3-5-sonnet-20241022     0      0      0.0%    \n",
            "4    technical_support    claude-3-5-sonnet-20241022     0      0      0.0%    \n",
            "5    escalation           claude-3-5-sonnet-20241022     0      1      0.0%    \n",
            "======================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "async def test_edge_case_all_busy():\n",
        "    \"\"\"Test what happens when all specialized agents are busy\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üß™ EDGE CASE TEST: All Specialized Agents Busy\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    agents = create_claude_agent_pool(async_client)\n",
        "    router = CapabilityMatchingRouter()\n",
        "\n",
        "    # Simulate all refund agents being at capacity\n",
        "    for agent in agents:\n",
        "        if agent.capability.agent_type == AgentType.REFUND:\n",
        "            agent.current_tasks = agent.capability.max_concurrent_tasks\n",
        "            print(f\"‚ö†Ô∏è  Agent {agent.agent_id} (REFUND) set to maximum capacity\")\n",
        "\n",
        "    # Try to route a refund query\n",
        "    test_query = Query(\n",
        "        id=999,\n",
        "        content=\"I need a refund urgently!\",\n",
        "        query_type=QueryType.REFUND,\n",
        "        priority=4,\n",
        "        timestamp=time.time()\n",
        "    )\n",
        "\n",
        "    routed_agent = router.route(test_query, agents)\n",
        "\n",
        "    if routed_agent:\n",
        "        print(f\"\\n‚úÖ Fallback Successful!\")\n",
        "        print(f\"   Query routed to: Agent {routed_agent.agent_id}\")\n",
        "        print(f\"   Agent Type: {routed_agent.capability.agent_type.value}\")\n",
        "        print(f\"   Strategy: {'Escalation Agent' if routed_agent.capability.agent_type == AgentType.ESCALATION else 'Alternative Agent'}\")\n",
        "\n",
        "        # Actually process the query\n",
        "        result = await routed_agent.process_query(test_query)\n",
        "\n",
        "        if result['success']:\n",
        "            print(f\"\\nüí¨ Response Preview:\")\n",
        "            print(f\"   {result['response'][:300]}...\")\n",
        "    else:\n",
        "        print(\"\\n‚ùå Routing failed - no fallback available\")\n",
        "\n",
        "# Run edge case test\n",
        "await test_edge_case_all_busy()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d3BEUQOYfcH",
        "outputId": "697e757b-52dd-411a-b1ad-99dd02e49830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "üß™ EDGE CASE TEST: All Specialized Agents Busy\n",
            "======================================================================\n",
            "\n",
            "‚ö†Ô∏è  Agent 2 (REFUND) set to maximum capacity\n",
            "‚ö†Ô∏è  Agent 3 (REFUND) set to maximum capacity\n",
            "\n",
            "‚úÖ Fallback Successful!\n",
            "   Query routed to: Agent 5\n",
            "   Agent Type: escalation\n",
            "   Strategy: Escalation Agent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_costs(results: List[dict]):\n",
        "    \"\"\"Estimate API costs based on token usage\"\"\"\n",
        "\n",
        "    # Pricing as of Nov 2025 (check current pricing at https://www.anthropic.com/pricing)\n",
        "    pricing = {\n",
        "        'claude-3-5-haiku-20241022': {\n",
        "            'input': 0.80 / 1_000_000,   # $0.80 per MTok\n",
        "            'output': 4.00 / 1_000_000    # $4.00 per MTok\n",
        "        },\n",
        "        'claude-3-5-sonnet-20241022': {\n",
        "            'input': 3.00 / 1_000_000,    # $3.00 per MTok\n",
        "            'output': 15.00 / 1_000_000   # $15.00 per MTok\n",
        "        }\n",
        "    }\n",
        "\n",
        "    total_cost = 0.0\n",
        "    token_breakdown = defaultdict(int)\n",
        "\n",
        "    for result in results:\n",
        "        if result.get('success') and 'tokens_used' in result:\n",
        "            model = result.get('model', 'claude-3-5-sonnet-20241022')\n",
        "            tokens = result['tokens_used']\n",
        "\n",
        "            # Approximate 60% input, 40% output split\n",
        "            input_tokens = int(tokens * 0.6)\n",
        "            output_tokens = int(tokens * 0.4)\n",
        "\n",
        "            if model in pricing:\n",
        "                cost = (input_tokens * pricing[model]['input'] +\n",
        "                       output_tokens * pricing[model]['output'])\n",
        "                total_cost += cost\n",
        "                token_breakdown[model] += tokens\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üí∞ COST ESTIMATION\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Total Estimated Cost: ${total_cost:.4f}\")\n",
        "    print(f\"\\nToken Usage by Model:\")\n",
        "    for model, tokens in token_breakdown.items():\n",
        "        print(f\"  {model}: {tokens:,} tokens\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    return total_cost\n",
        "\n",
        "# Run cost estimation on previous results\n",
        "if 'results' in locals():\n",
        "    estimate_costs(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk3cCIK2YtSf",
        "outputId": "9a4f05bc-5f6e-4850-86bb-9381fb581726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "üí∞ COST ESTIMATION\n",
            "======================================================================\n",
            "Total Estimated Cost: $0.0012\n",
            "\n",
            "Token Usage by Model:\n",
            "  claude-3-5-haiku-20241022: 574 tokens\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}